\section{Wykład 3 - liniowe i nieliniowe sieci neuronowe $\heartsuit$ $\heartsuit$ $\heartsuit$  }

\begin{enumerate}
 \item jądro - centrum obliczeniowe neuronu, to są kluczowe procesy
 \item akson - wyjście neuronu, ma tylko jedno wyjście
 \item wzgórek aksonu - sumowanie przychodzących sygnałów i generowanie potencjałów czynnościowych, które
 wędrują przez akson
 \item dendryt - wejście neuronu, może być wiele, biologiczne neurony mają ich tysiące
 \item synapsa - jeśli dendryt jest wejsciem neuronu to synapsa jest furtką. ma wpływ na moc sygnału napływającego poprzez akson.
\end{enumerate}

\paragraph{Ogólna definicja neuronu}

\begin{equation}
 y = f(\phi(x_i w_i))
\end{equation}

\begin{enumerate}
 \item $\phi$ - Post synaptic potential (potencjał postsynaptyczny)
 \item $f$ - funkcja aktywacji
 \item $x_i$ - wejścia
 \item $w_i$ - wagi
 \item $y$ - wyjście
\end{enumerate}

\paragraph{Definicja neuronu}

\begin{equation}
 y = f(\sum(x_i w_i))
\end{equation}

Zamiast PSP blok sumujący.

\paragraph{Neuron liniowy}

\begin{equation}
 y = \sum(x_i w_i)
\end{equation}

Brak funkcji aktywacji.

\paragraph{Neuron z funkcją skokową (nieliniowy neuronik)}

$f(x)$ jest takie, że jeśli $x$ jest większe od progu to robi skok (podaje $1$), inaczej $0$.

\paragraph{BIAS (przesunięcie) oraz PRÓG (threshold)}

\begin{equation}
 e = b + \sum_{i=1}^n x_i w_i
\end{equation}

Można przyjąć, że $w_0 = b$ i wtedy bias jest jak każda inna waga, z tym, że sygnał $x_0$ jest równy $1$.
Wtedy:
\begin{equation}
 e = \sum_{i=0}^n x_i w_i
\end{equation}

Funkcję aktywacji można wtedy przyjąć jako:

\begin{equation}
  f(e)= 
\begin{cases}
    1,& \text{if } e \ge 0 \\
    0,              & \text{otherwise}
\end{cases}
\end{equation}

Możemy uznać, że bias jest stałym progiem wtedy:

\begin{equation}
 e = \sum_{i=1}^n x_i w_i
\end{equation}

oraz

\begin{equation}
  f(e)= 
\begin{cases}
    1,& \text{if } e \ge \theta \\
    0,              & \text{otherwise}
\end{cases}
\end{equation}

gdzie $\theta$ to próg.

Pierwszy wniosek jest taki, że $\sum x_i w_i$ jest prostą, płaszczyzną, i hiperpłaszczyzną (zależy od wymiaru).

Druga różnica jest taka, że przy założeniu, że $w_2$ jest różne od $0$ wzory na granicę pomiędzy $0$, a $1$ na wyjściu są takie:
(dla 2 wymiarów)
\begin{equation}
 x_2 = - \frac{w_1}{w_2} - \frac{b}{w_2}
\end{equation}

oraz

\begin{equation}
 x_2 = - \frac{w_1}{w_2} + \frac{\theta}{w_2}
\end{equation}

Jak jest próg stały to uwaga... podczas nauki się nie zmienia! A jak nie jest stały to uwaga.. się zmienia!
I to czy ma być stały czy nie zależy od problemu. Bo raz zadziała, raz nie... heheszki.

Dyskryminacja liniowa - wykres przedstawiający linię oddzielającą $0$ od $1$ dla różnych $x$
(np. w 2d)

Powierzchnia odpowiedzi - dodajemy 3 wykres - wartość wyjścia - dyskryminacja liniowa jest rzutem na powierzchnię odpowiedzi od góry na iksy

\paragraph{Neuron liniowy - screeny od Tadka}

Wejścia i wyjścia w neuronie mogą być znormalizowane (patrz Tadeusiewicz, ale lepiej nie) od $-1$ do $1$.

I można zapisać wzory wektorowo:
\begin{equation}
 y = W^T X
\end{equation}

co jest najzwyczajniejszym na świecie mnożeniem iksy przez wartości. Jednak jeśli $x_i$ i $w_i$ są
znormalizowane to 
\begin{equation}
 y = cos \phi
\end{equation}

gdzie $\phi$ jest kątem pomiędzy wektorami $W$ oraz $X$. Prosty wniosek, że sygnał będzie tym większy
im bardziej kąt będzie mniejszy (czyli wektory będą skierowane bardziej w tą samą stronę).

Można też zapisać to macierzowo.

W sieci liniowej nie robi się warstw ukrytych z zasady, bo nie wzbogacą zachowania sieci.
(algebra)

\paragraph{ADALINE - Adaptive Linear Element - Reguła Widrow-Hoffa}

Regułę robimy następująco:
\begin{equation}
 W' = W + \mu \delta X
\end{equation}

albo za pomocą wzoru 

\begin{equation}
 W' = Z X^{-1}
\end{equation}

$mu \ge 0$. Jak $\delta = z - y$ jest $\ge 0$ to znaczy, że $z > y$ czyli odpowiedź sieci była za mała.
A jak $\delta < 0 $ to $ z < y $ czyli odpowiedź była za duża.

Jak odpowiedź była za mała to $W'$ się zwiększa, jak za duża to $W'$ się zmniejsza.

Można udowodnić, że reguła ta optymalizuje wagi względem funkcji celu:
\begin{equation}
 Q = \frac{1}{2} \sum_{i=1}^N {(z^j - y^j)}^2
\end{equation}
za pomocą poszukiwania minimum metodą gradientową.

MADALINE to jest Wiele ADALINE i Reguła Widrow-Hoffa również działa.